---
### **Propósito:** REGRESION LINEAL.
```
---
title: "04 - Regresión Lineal"
output: html_document
---

```{r}
source("../origen/config.R")
source("../origen/data_utils.R")
```

##Librerías

```{r setup, include=FALSE}

# Instalar y cargar pacman (gestor de paquetes)
knitr::opts_chunk$set(echo = TRUE)

if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(
  tidyverse, readr, janitor, skimr, naniar, DataExplorer,
  GGally, ggcorrplot, lubridate, dplyr, tidyr, stringr, stringi, purrr, tibble, here, webshot2, gt, osmdata, sf, tidyverse, here, units, progressr,osmextract, rosm, ggspatial, prettymapr, nngeo, RANN, textclean, spatialsample, blockCV, tidymodels, SuperLearner, nnls, recipes, Xgboost, ranger, glmnet, gt, tibble, caret
)

# Verificar paquetes cargados
pacman::p_loaded()


here::i_am("notebooks/04_lineal regression.rmd")

```



####### Regresión lineal #######
# 1. CARGA Y PREPARACIÓN DE DATOS 

```{r}

train_final <- readRDS(here::here("data", "processed", "train_final.rds"))

test_final <- readRDS(here::here("data", "processed", "test_final.rds"))


```


```{r}



train_final <- readRDS(here::here("data", "processed", "train_final.rds"))
test_final  <- readRDS(here::here("data", "processed", "test_final.rds"))


# LIMPIEZA DE OUTLIERS Y PREPARACIÓN DEL TRAIN

q_low  <- quantile(train_final$price, 0.01, na.rm = TRUE)
q_high <- quantile(train_final$price, 0.99, na.rm = TRUE)

vivienda_bogota <- train_final %>%
  filter(price >= q_low & price <= q_high) %>%
  mutate(
    preciom = price / 1e6,                   # precio en millones
    areaconst = surface_total                # renombrar superficie
  ) %>%
  filter(!is.na(preciom), !is.na(areaconst)) # limpiar NAs


# PREPARACIÓN DEL TEST SET


median_area_train <- median(vivienda_bogota$areaconst, na.rm = TRUE)

test_learner <- test_final %>%
  mutate(
    preciom = NA,                             # test no tiene precio
    areaconst = surface_total                 # renombrar igual que train
  ) %>%
  mutate(
    areaconst = ifelse(is.na(areaconst), median_area_train, areaconst)
  )



```



# 2.  AJUSTE Y ANÁLISIS DE DIFERENTES MODELOS LINEALES

```{r}
modelo1 <- lm(preciom ~ areaconst, data = vivienda_bogota)

# Resumen del modelo
print(summary(modelo1))

# Intervalos de confianza para los coeficientes

print(confint(modelo1))
```

```{r}

# Añadimos 1 para evitar log(0) si hubiera áreas de 0
modelo2 <- lm(log(preciom) ~ log(areaconst + 1), data = vivienda_bogota)
print(summary(modelo2))
```

```{r}

modelo3 <- lm(log(preciom) ~ areaconst, data = vivienda_bogota)
print(summary(modelo3))
```


```{r}

modelo4 <- lm(preciom ~ log(areaconst + 1), data = vivienda_bogota)
print(summary(modelo4))
```


# 3. COMPARACIÓN DE MODELOS Y SELECCIÓN DEL MEJOR

```{r}
# -- COMPARACIÓN CON AIC --
# El Criterio de Información de Akaike (AIC) nos ayuda a comparar modelos.
# El modelo con el AIC más bajo es el preferido.
aic_results <- AIC(modelo1, modelo2, modelo3, modelo4)

print(aic_results)

# Seleccionamos el mejor modelo (el que tiene el menor AIC)
best_model_name <- rownames(aic_results)[which.min(aic_results$AIC)]

best_simple_model <- get(best_model_name)
```

# 4. PREDICCIÓN CON EL MEJOR MODELO SIMPLE

```{r}


# Verificamos si el mejor modelo predice el logaritmo del precio
predicts_log <- str_detect(as.character(formula(best_simple_model)), "log") |> any()
predicts_log <- isTRUE(predicts_log)   # Garantiza longitud 1

# Hacemos la predicción en el set de prueba
predictions_simple_raw <- predict(best_simple_model, newdata = test_learner)

# Convertimos las predicciones a la escala original de precios
if (predicts_log) {
  # Si el modelo predijo log(preciom), revertimos la transformación
  precios_finales_simple <- exp(predictions_simple_raw) * 1e6
} else {
  # Si el modelo predijo preciom, solo multiplicamos por 1 millón
  precios_finales_simple <- predictions_simple_raw * 1e6
}

# Creamos el dataframe de submission
submission_simple <- tibble(
  property_id = test_learner$property_id,
  price = precios_finales_simple
)

# Guardamos el archivo
ruta_destino <- here::here("outputs", "kaggle_submissions")
dir.create(ruta_destino, showWarnings = FALSE, recursive = TRUE)
ruta_archivo_submission <- here::here(ruta_destino, "submission_simple_linear.csv")
write_csv(submission_simple, ruta_archivo_submission)


summary(submission_simple$price)

```

# 5. MODELO DE REGRESIÓN MÚLTIPLE

```{r}
# Un modelo con una sola variable es una buena base, pero para una mejor
# predicción, debemos usar más información.



# Seleccionamos un conjunto más rico de variables
features_multiples <- c(
  "surface_total", "bedrooms", "bathrooms", "estrato",
  "latitude", "longitude", "property_type", "localidad"
)

# Creamos la fórmula SIN transformación en la variable respuesta
model_formula_multi <- as.formula(
  paste("price ~", paste(features_multiples, collapse = " + "))
)

# Creamos la receta, aplicando log(price) correctamente
rec_multi <- recipe(model_formula_multi, data = train_full) %>%
  step_log(price, skip = TRUE) %>%                               
  step_other(localidad, threshold = 0.01) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_zv(all_predictors())

# Especificamos el modelo, lo unimos al workflow y lo entrenamos
lm_spec <- linear_reg() %>% set_engine("lm")
lm_workflow <- workflow() %>% add_recipe(rec_multi) %>% add_model(lm_spec)
final_lm_fit <- fit(lm_workflow, data = train_full)

# Predicción con el Modelo Múltiple 


log_predictions_multi <- predict(final_lm_fit, new_data = test_full)

# Convertimos de logaritmo a precio original
precios_finales_multi <- exp(log_predictions_multi$.pred)

# Creamos el dataframe de submission
submission_multiple <- tibble(
  property_id = test_full$property_id,
  price = precios_finales_multi
)

# Guardamos el archivo
ruta_archivo_submission_multi <- here::here(ruta_destino, "submission_multiple_linear.csv")
write_csv(submission_multiple, ruta_archivo_submission_multi)


summary(submission_multiple$price)

```


## Con receta de procesamiento avanzada

```{r}


# -------------------------------
# 1) VARIABLES
# -------------------------------

features <- c(
  "surface_total", "bedrooms", "bathrooms", "estrato",
  "latitude", "longitude", "property_type", "localidad",
  "has_parqueadero_garaje", "has_ascensor", "has_terraza"
)

vars_necesarias_test <- c(features, "property_id", "surface_covered","dist_autopistas","dist_centros_comerciales", "dist_parques")

# Reconstruimos test_learner GARANTIZANDO todas las columnas
test_learner <- test_final[, vars_necesarias_test, drop = FALSE]

# -------------------------------
# 2) ASEGURAR QUE TRAIN Y TEST TIENEN MISMO NIVELES FACTOR
# -------------------------------


categorical_features <- c("property_type", "localidad", "estrato")

for (v in categorical_features) {
  lvls <- union(levels(as.factor(train_final[[v]])),
                levels(as.factor(test_final[[v]])))
  train_final[[v]] <- factor(train_final[[v]], levels = lvls)
  test_final[[v]]  <- factor(test_final[[v]],  levels = lvls)
}


# -------------------------------
# 3) CREAR FORMULA CORRECTA
# -------------------------------

model_formula <- as.formula(
  paste("price ~", paste(features, collapse = " + "))
)

# -------------------------------
# 4) RECIPE AVANZADA
# -------------------------------

rec_avanzada <- recipe(model_formula, data = train_final) %>%
  
  # Transformación de la variable objetivo (solo en training)
  step_log(price, skip = TRUE) %>%
  
  # Transformación log predictores sesgados
  step_log(surface_total, offset = 1) %>%
  
  # Winsorizing ESTABLE (basado en percentiles del training)
  step_mutate_at(
    surface_total,
    fn = function(x) {
      p <- quantile(x, 0.98, na.rm = TRUE)
      pmin(x, p)
    }
  ) %>%
  
  # Categóricas
  step_other(localidad, threshold = 0.02, other = "otra_localidad") %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  
  # Interacciones
  step_interact(terms = ~ starts_with("surface_total") : starts_with("estrato_")) %>%
  step_interact(terms = ~ starts_with("surface_total") : starts_with("localidad_")) %>%
  
  # Limpieza final
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())

# No se usa prep(): workflows lo hacen automáticamente
# prep_rec <- prep(rec_avanzada, training = train_learner)  # ← SE ELIMINA


# WORKFLOW + MODELO


lm_spec <- linear_reg() %>% set_engine("lm")

lm_workflow <- workflow() %>%
  add_recipe(rec_avanzada) %>% 
  add_model(lm_spec)


# ENTRENAMIENTO

final_fit <- fit(lm_workflow, data = train_final)



# PREDICCIÓN FINAL



log_predictions <- predict(final_fit, new_data = test_final)

# Convertimos de log a la escala original
precios_finales <- exp(log_predictions$.pred)

# -------------------------------
# 8) CREAR SUBMISSION
# -------------------------------

submission <- tibble(
  property_id = test_learner$property_id,
  price = precios_finales
)

ruta_destino <- here::here("outputs", "kaggle_submissions")
dir.create(ruta_destino, showWarnings = FALSE, recursive = TRUE)

ruta_archivo_submission <- here::here(ruta_destino, "submission_lm_potenciado.csv")
write_csv(submission, ruta_archivo_submission)

message(paste0("\nArchivo de submission guardado en:\n", ruta_archivo_submission))

cat("\n--- Resumen de Precios Predichos (Modelo Potenciado) ---\n")
summary(submission$price)

cat("\n--- Resumen en Millones de Pesos ---\n")
summary(submission$price / 1e6)

# -------------------------------
# 9) Histograma
# -------------------------------
ggplot(submission, aes(x = price / 1e6)) +
  geom_histogram(bins = 50, fill = "deepskyblue3", alpha = 0.7) +
  geom_vline(xintercept = 200, color = "red", linetype = "dashed", size = 1) +
  labs(
    title = "Distribución de Precios Predichos",
    subtitle = "La línea roja marca los 200 millones",
    x = "Precio Predicho (en Millones de Pesos)",
    y = "Frecuencia"
  ) +
  theme_minimal()

```











