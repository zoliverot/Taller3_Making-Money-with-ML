
---
title: "Problem Set 3 - BDML"
author: "Cristian Muñoz - Vivian Cabanzo - Zeneth Olivero - Laura Diaz"
date: "2025-11-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(
  tidyverse, readr, janitor, skimr, naniar, DataExplorer,
  GGally, ggcorrplot, lubridate, dplyr, tidyr, stringr, stringi, purrr, tibble
)
```

# 1. Limpieza de datos
```{r}
test <- read_csv("https://raw.githubusercontent.com/cristianmu910/datos-taller-3/main/test.csv")
train <- read_csv("https://raw.githubusercontent.com/cristianmu910/datos-taller-3/main/train.csv")

theme_set(theme_minimal())

# Opcional: normalizar nombres a snake_case
train <- janitor::clean_names(train)
test  <- janitor::clean_names(test)

# Define tu objetivo (ajusta si se llama distinto)
target <- "price"

# ===============================
# 2) Estructura y diferencias básicas
# ===============================
cat("\nDimensiones:\n")
cat("train: ", paste(dim(train), collapse=" x "), "\n")
cat("test : ", paste(dim(test),  collapse=" x "), "\n")

train_cols <- names(train)
test_cols  <- names(test)

cols_common        <- intersect(train_cols, test_cols)
cols_train_only    <- setdiff(train_cols, test_cols)
cols_test_only     <- setdiff(test_cols, train_cols)
cols_common_nontgt <- setdiff(cols_common, target)

cat("\nColumnas SOLO en train (excluyendo target):\n")
print(setdiff(cols_train_only, target))
cat("\nColumnas SOLO en test:\n")
print(cols_test_only)

cat("\nPrimeras columnas comunes:\n")
print(head(cols_common, 20))

# ===============================
# 3) Tipos de datos y vistazo
# ===============================
cat("\nGlimpse train:\n")
glimpse(train)
cat("\nGlimpse test:\n")
glimpse(test)

# Resumen amigable
cat("\nResumen SKIM (train):\n")
skimr::skim(train)

# ===============================
# 4) Chequeos de duplicados
# ===============================
# Si tienes un id (p.ej. property_id) cambia aquí:
id_col <- dplyr::case_when(
  "property_id" %in% names(train) ~ "property_id",
  TRUE ~ NA_character_
)

if (!is.na(id_col)) {
  dup_train <- train %>% count(.data[[id_col]]) %>% filter(n > 1) %>% nrow()
  dup_test  <- test  %>% count(.data[[id_col]]) %>% filter(n > 1) %>% nrow()
  cat("\nDuplicados por ID en train:", dup_train, " | en test:", dup_test, "\n")
} else {
  cat("\n(No se detectó columna ID típica; se revisan duplicados de filas completas)\n")
  cat("Filas duplicadas en train:", sum(duplicated(train)), "\n")
  cat("Filas duplicadas en test :", sum(duplicated(test)),  "\n")
}

# ===============================
# 5) Faltantes (NA) y comparaciones
# ===============================
cat("\nFaltantes por variable (train):\n")
miss_train <- naniar::miss_var_summary(train) %>% rename(n_miss_train = n_miss, pct_miss_train = pct_miss)
print(head(miss_train, 15))

cat("\nFaltantes por variable (test):\n")
miss_test <- naniar::miss_var_summary(test) %>% rename(n_miss_test = n_miss, pct_miss_test = pct_miss)
print(head(miss_test, 15))

# Comparación NA train vs test (solo columnas comunes)
miss_compare <- full_join(miss_train %>% rename(variable = variable),
                          miss_test  %>% rename(variable = variable),
                          by = "variable") %>%
  mutate(across(starts_with("pct_miss"), ~replace_na(., 0))) %>%
  arrange(desc(abs(pct_miss_train - pct_miss_test)))

cat("\nTop 15 diferencias de faltantes (train vs test):\n")
print(head(miss_compare, 15))

# Visual rápido de NA
naniar::vis_miss(train %>% select(all_of(cols_common_nontgt))) +
  ggtitle("Mapa de faltantes - TRAIN (solo columnas comunes sin target)")

naniar::vis_miss(test %>% select(all_of(cols_common_nontgt))) +
  ggtitle("Mapa de faltantes - TEST (solo columnas comunes sin target)")

DataExplorer::plot_missing(train %>% select(all_of(cols_common_nontgt))) +
  ggtitle("Proporción de faltantes por variable - TRAIN")

DataExplorer::plot_missing(test %>% select(all_of(cols_common_nontgt))) +
  ggtitle("Proporción de faltantes por variable - TEST")

# ===============================
# 6) Detectar numéricas vs categóricas
# ===============================
is_num <- function(x) is.numeric(x) || inherits(x, c("integer","numeric","double"))
num_cols <- train %>%
  select(any_of(cols_common_nontgt)) %>%
  select(where(is_num)) %>% names()

cat_cols <- setdiff(cols_common_nontgt, num_cols)

cat("\nNuméricas:\n"); print(num_cols)
cat("\nCategóricas:\n"); print(cat_cols)

# ===============================
# 7) Distribuciones (gráficos sencillos)
# ===============================

# 7.1 Histograma para cada numérica (train)
if (length(num_cols) > 0) {
  train %>%
    select(all_of(num_cols)) %>%
    pivot_longer(everything(), names_to = "variable", values_to = "valor") %>%
    ggplot(aes(x = valor)) +
    geom_histogram(bins = 30) +
    facet_wrap(~ variable, scales = "free", ncol = 3) +
    labs(title = "Histogramas (TRAIN)", x = NULL, y = "Frecuencia")
}

# 7.2 Densidad del target (train), si existe
if (target %in% names(train)) {
  # Densidad en escala original
  ggplot(train, aes(x = .data[[target]])) +
    geom_density() +
    labs(title = paste("Densidad de", target, "(TRAIN)"), x = target, y = "Densidad")

  # Densidad en log (opcional)
  if (all(is.finite(train[[target]]), na.rm = TRUE)) {
    ggplot(train %>% filter(.data[[target]] > 0), aes(x = log1p(.data[[target]]))) +
      geom_density() +
      labs(title = paste("Densidad de log1p(", target, ") (TRAIN)"), x = paste0("log1p(", target, ")"), y = "Densidad")
  }
}

# 7.3 Boxplots del target por algunas categóricas (top k categorías más frecuentes)
top_k_levels <- function(x, k = 12) {
  forcats::fct_lump_n(x, n = k, other_level = "otros")
}

if (target %in% names(train) && length(cat_cols) > 0) {
  for (cc in cat_cols) {
    p <- train %>%
      mutate(..grp = top_k_levels(.data[[cc]], k = 12)) %>%
      ggplot(aes(x = ..grp, y = .data[[target]])) +
      geom_boxplot(outlier.alpha = 0.2) +
      coord_flip() +
      labs(title = paste("Boxplot de", target, "por", cc, "(TRAIN)"),
           x = cc, y = target)
    print(p)
  }
}

# 7.4 Pares de variables (scatter) para algunas numéricas clave
pairs_cols <- intersect(num_cols, c("bathrooms","bedrooms","rooms","surface_total","surface_covered","lat","lon","year","month"))
if (length(pairs_cols) >= 2 && target %in% names(train)) {
  df_pairs <- train %>% select(all_of(pairs_cols), all_of(target)) %>% drop_na()
  GGally::ggpairs(df_pairs, columns = 1:length(pairs_cols),
                  title = "Matriz de pares (subset numérico) - TRAIN")
}

# ===============================
# 8) Correlaciones numéricas
# ===============================
if (length(num_cols) >= 2) {
  # correlaciones entre numéricas (train)
  cormat <- train %>% select(all_of(num_cols)) %>%
    mutate(across(everything(), as.numeric)) %>%
    cor(use = "pairwise.complete.obs")

  ggcorrplot::ggcorrplot(cormat, lab = FALSE, type = "lower") +
    ggtitle("Correlación entre variables numéricas (TRAIN)")

  # correlación con target si numérico
  if (target %in% names(train) && is.numeric(train[[target]])) {
    cor_with_target <- train %>%
      select(all_of(num_cols), all_of(target)) %>%
      cor(use = "pairwise.complete.obs") %>%
      as.data.frame() %>%
      rownames_to_column("variable") %>%
      select(variable, all_of(target)) %>%
      arrange(desc(abs(.data[[target]])))
    cat("\nCorrelación con el target (TRAIN):\n")
    print(cor_with_target)
  }
}

# ===============================
# 9) Recomendación de columnas a modelar
# ===============================
keep_features <- setdiff(cols_common, target)  # lo común menos la y
cat("\nFeatures a usar (comunes, sin target):\n")
print(keep_features)
```
# Análisis de texto
```{r}
# --- 1) Normalizador de texto (lower + sin tildes + espacios compactados) ---
normalize_text <- function(x) {
  x %>%
    tidyr::replace_na("") %>%
    stringi::stri_trans_general("Latin-ASCII") %>% # quita tildes
    tolower() %>%
    stringr::str_squish()
}

# --- 2) Diccionario de atributos (regex) para dummies desde title+description ---
# Puedes ajustar/expandir vocabulario según tu mercado
attr_patterns <- list(
  ubic_parque          = "\\bparques?\\b(?!adero)",                             # parque, no parqueadero
  parqueadero_garaje   = "\\b(parqueadero|garaje|estacionamiento)s?\\b",
  transporte_metro_tm  = "\\b(metro|transmilenio|sitp|subte|estacion|paradero)\\b",
  colegio_universidad  = "\\b(colegios?|universidades?|jardin(?:es)? infantil(?:es)?)\\b",
  salud_hospital       = "\\b(hospital(es)?|clinica(s)?|centro(s)? medico(s)?)\\b",
  centro_comercial     = "\\b(centro(s)? comercial(es)?)\\b",
  supermercado         = "\\b(supermercado(s)?|tienda(s)?|exito|carulla|jumbo|olimpica|d1|ara)\\b",
  seguridad            = "\\b(vigilancia|porteri[ai]|seguridad|camaras?)\\b",
  amenities            = "\\b(piscina|gimnasio|bbq|sal[oó]n social|zona social|zona infantil)\\b",
  ascensor             = "\\bascensor(es)?\\b",
  balcon               = "\\bbalcon(es)?\\b",
  terraza              = "\\bterraza(s)?\\b",
  remodelado           = "\\b(remodelad[oa]s?|remodelaci[oó]n)\\b",
  cocina_integral      = "\\bcocina integral\\b",
  iluminacion          = "\\biluminaci[oó]n\\b",
  ventilacion          = "\\bventilaci[oó]n\\b",
  patio                = "\\bpatio(s)?\\b",
  estudio              = "\\bestudio(s)?\\b",
  deposito             = "\\bdepo[si]t[oó]s?\\b|\\bbaulera(s)?\\b|\\btrastero(s)?\\b",
  chimenea             = "\\bchimenea(s)?\\b",
  walk_in_closet       = "\\b(vestier|walk(?:-|\\s*)in\\s*closet)\\b",
  vista                = "\\b(vista|panor[aá]mica)\\b",
  exterior_interior    = "\\b(exterior|interior|esquinero|esquina)\\b",
  estrato              = "\\bestrato\\s*[1-6]\\b"
)

# --- 3) Función: construir dummies desde texto ---
add_text_flags <- function(df, patterns = attr_patterns) {
  stopifnot(all(c("title","description") %in% names(df)))
  txt <- normalize_text(paste(df$title, df$description))
  out <- df
  for (nm in names(patterns)) {
    rx <- stringr::regex(patterns[[nm]], ignore_case = TRUE)
    out[[paste0("has_", nm)]] <- as.integer(stringr::str_detect(txt, rx))
  }
  out
}

# --- 4) Aplica procesamiento a train/test ---
train_txt <- add_text_flags(train)
test_txt  <- add_text_flags(test)

# --- 5) Evaluación univariada vs price en TRAIN (cobertura, efecto, %Δ, eta²) ---
target <- "price"

score_one_attr <- function(df, attr_col, target = "price") {
  # grupos por flag
  g <- df |>
    dplyr::summarise(
      mean1 = mean(.data[[target]][.data[[attr_col]] == 1], na.rm = TRUE),
      mean0 = mean(.data[[target]][.data[[attr_col]] == 0], na.rm = TRUE),
      n1    = sum(.data[[attr_col]] == 1, na.rm = TRUE),
      n0    = sum(.data[[attr_col]] == 0, na.rm = TRUE),
      .groups = "drop"
    )
  effect   <- g$mean1 - g$mean0
  pct_diff <- ifelse(is.finite(g$mean0) && g$mean0 != 0, effect / g$mean0, NA_real_)
  # eta² (varianza explicada por 2 grupos)
  grand    <- mean(df[[target]], na.rm = TRUE)
  ss_between <- g$n1 * (g$mean1 - grand)^2 + g$n0 * (g$mean0 - grand)^2
  ss_total   <- sum((df[[target]] - grand)^2, na.rm = TRUE)
  eta2 <- ifelse(ss_total > 0, ss_between / ss_total, 0)
  tibble(
    attribute = attr_col,
    support_1 = g$n1,
    rate_1    = (g$n1)/(g$n1 + g$n0),
    mean_price_1 = g$mean1,
    mean_price_0 = g$mean0,
    mean_diff    = effect,
    pct_diff     = pct_diff,
    eta2         = eta2
  )
}

flag_cols <- names(train_txt) |> (\(v) v[stringr::str_starts(v, "has_")])()
stats_tbl <- purrr::map_dfr(flag_cols, ~score_one_attr(train_txt, .x, target = target))

# Ranking: cobertura mínima, tamaño de efecto y eta² (ajusta pesos si quieres)
min_rate <- 0.03  # >= 3% de menciones
ranked <- stats_tbl |>
  mutate(rank_score = eta2 * 0.5 + abs(pct_diff) * 0.3 + rate_1 * 0.2) |>
  filter(rate_1 >= min_rate) |>
  arrange(desc(rank_score))

# --- 6) Reducir redundancia entre dummies (phi) y elegir TOP-4 ---
phi_bin <- function(a, b) {
  pa <- mean(a); pb <- mean(b)
  cov <- mean(a*b) - pa*pb
  denom <- sqrt(pa*(1-pa)*pb*(1-pb))
  if (!is.finite(denom) || denom == 0) return(0)
  cov/denom
}

# matriz phi en candidatas
cand <- ranked$attribute
Phi <- matrix(NA_real_, nrow = length(cand), ncol = length(cand), dimnames = list(cand, cand))
for (i in seq_along(cand)) {
  for (j in seq_along(cand)) {
    if (i == j) { Phi[i, j] <- 1; next }
    a <- as.integer(train_txt[[cand[i]]]); b <- as.integer(train_txt[[cand[j]]])
    Phi[i, j] <- phi_bin(a, b)
  }
}

picked <- character(0)
for (nm in ranked$attribute) {
  if (length(picked) == 4) break
  ok <- all(abs(Phi[nm, picked, drop = FALSE]) <= 0.8)
  if (ok) picked <- c(picked, nm)
}

top4 <- picked
top4
#> vector con los 4 "has_*" seleccionados automáticamente

# --- 7) Crear SOLO esas 4 dummies finales coherentes en train/test ---
add_top4 <- function(df, final_names) {
  # Asegura columnas aunque alguna no aparezca en test
  for (nm in final_names) if (!nm %in% names(df)) df[[nm]] <- 0L
  df |>
    mutate(across(all_of(final_names), ~as.integer(.))) 
}

train_ready <- add_top4(train_txt, top4)
test_ready  <- add_top4(test_txt,  top4)

# --- 8) Pequeñas tablas para el informe (justificación reproducible) ---
justificacion <- ranked |>
  filter(attribute %in% top4) |>
  select(attribute, support_1, rate_1, mean_price_1, mean_price_0, mean_diff, pct_diff, eta2, rank_score)

justificacion |>
  arrange(desc(rank_score)) |>
  mutate(pct_diff = scales::percent(pct_diff, accuracy = 0.1),
         rate_1   = scales::percent(rate_1, accuracy = 0.1)) |>
  select(attribute, rate_1, mean_price_1, mean_price_0, pct_diff, eta2)

```


# Variables Internas (4 variables que salen de la columna de texto en ambas bases de datos)
```{r}

# Normaliza: minúsculas, sin tildes, espacios compactados
normalize_text <- function(x) {
  x %>%
    tidyr::replace_na("") %>%
    stringi::stri_trans_general("Latin-ASCII") %>%
    tolower() %>%
    stringr::str_squish()
}

# Crea las 4 dummies desde title + description
add_text_dummies <- function(df) {
  stopifnot(all(c("title","description") %in% names(df)))
  
  txt <- normalize_text(paste(df$title, df$description))
  
  # Patrones (regex) — puedes ajustar si quieres ampliar vocabulario
  rx_parqueadero_garaje <- stringr::regex("\\b(parqueadero|garaje|estacionamiento)s?\\b", ignore_case = TRUE)
  rx_cocina_integral    <- stringr::regex("\\bcocina integral\\b", ignore_case = TRUE)
  rx_estudio            <- stringr::regex("\\bestudio(s)?\\b", ignore_case = TRUE)
  rx_chimenea           <- stringr::regex("\\bchimenea(s)?\\b", ignore_case = TRUE)
  
  df %>%
    mutate(
      has_parqueadero_garaje = as.integer(stringr::str_detect(txt, rx_parqueadero_garaje)),
      has_cocina_integral    = as.integer(stringr::str_detect(txt, rx_cocina_integral)),
      has_estudio            = as.integer(stringr::str_detect(txt, rx_estudio)),
      has_chimenea           = as.integer(stringr::str_detect(txt, rx_chimenea))
    )
}

# === Uso ===
# train ya cargado como data.frame/tibble con columnas title y description
train_ready <- add_text_dummies(train)
test_ready <- add_text_dummies(test)

# (Opcional) ver cobertura de cada dummie en train
colMeans(dplyr::select(train_ready, starts_with("has_")))
colMeans(dplyr::select(test_ready, starts_with("has_")))

```

