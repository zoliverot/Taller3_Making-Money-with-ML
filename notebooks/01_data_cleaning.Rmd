
---
title: "Problem Set 3 - BDML"
author: "Cristian Mu√±oz - Vivian Cabanzo - Zeneth Olivero - Laura Diaz"
date: "2025-11-23"
output: html_document
---

# 1. DATOS. 

##Librer√≠as

```{r setup, include=FALSE}

# Instalar y cargar pacman (gestor de paquetes)
knitr::opts_chunk$set(echo = TRUE)

if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(
  tidyverse, readr, janitor, skimr, naniar, DataExplorer,
  GGally, ggcorrplot, lubridate, dplyr, tidyr, stringr, stringi, purrr, tibble, here, webshot2, gt, osmdata, sf, tidyverse, here, units, progressr,osmextract, rosm, ggspatial, prettymapr, nngeo, RANN, textclean, spatialsample, blockCV, tidymodels
)



# Verificar paquetes cargados
pacman::p_loaded()
```


## Cargar  bases de datos principales
```{r}
#Base de datos test
test <- read_csv("https://raw.githubusercontent.com/cristianmu910/datos-taller-3/main/test.csv")
##Base de datos train
train <- read_csv("https://raw.githubusercontent.com/cristianmu910/datos-taller-3/main/train.csv")
```


## Limpieza de datos

```{r}
#Configuraci√≥n inicial

#Establecemos visuales minimalistas
theme_set(theme_minimal())

# Normalizar nombres a snake_case
train <- janitor::clean_names(train)
test  <- janitor::clean_names(test)

# Objetivo a predecir
target <- "price"

```

## Exploraci√≥n  y manipulaci√≥n de datos principales 

```{r}

# Estructura y diferencias b√°sicas entre train y test

#diferencias y similutes
cat("\nDimensiones:\n")
cat("train: ", paste(dim(train), collapse=" x "), "\n") #38644 observaciones
cat("test : ", paste(dim(test),  collapse=" x "), "\n") #10286 observaciones
#Tienen las misma cantidad de variables

#Reviamos nombres
train_cols <- names(train)
test_cols  <- names(test)

#Diferencia en variables
vars_comunes <- intersect(names(train), names(test))
vars_train   <- setdiff(names(train), names(test))
vars_test    <- setdiff(names(test), names(train))
# Variables exclusivas en test (texto)
solo_test <- if (length(vars_test) == 0) "Ninguna variable" else paste(vars_test, collapse = ", ")

#  Construir tabla resumen
comp_bd_prop <- tibble(
  Categoria = c("En ambos datasets", "Solo en train", "Solo en test"),
  Cantidad  = c(length(vars_comunes), length(vars_train), length(vars_test)),
  Variables = c(
    paste(vars_comunes, collapse = ", "),
    ifelse(length(vars_train) == 0, "Ninguna variable", paste(vars_train, collapse = ", ")),
    solo_test
  )
)


# Formatear tabla con gt
tabla_comp_prop <- comp_bd_prop %>%
  gt() %>%
  tab_header(
    title = md("**Resumen de Variables entre la muestra de entrenamiento y de prueba (Propiedades)**")
  ) %>%
  cols_label(
    Categoria = "Categor√≠a",
    Cantidad = "N¬∞ de Variables",
    Variables = "Variables"
  ) %>%
  tab_options(
    table.font.size = 12,
    table.align = "left",
    heading.align = "center"
  )

#Gurdamos y exportamos

root_dir <- here::here()
path_outputs <- file.path(root_dir, "outputs")
path_tables <- file.path(path_outputs, "Tablas_y_Graficos")

dir.create(path_outputs, showWarnings = FALSE)
dir.create(path_tables, showWarnings = FALSE)

#  Exportar tabla en HTML, PDF y PNG
html_path <- file.path(path_tables, "Comparacion_bases_propiedades.html")
pdf_path  <- file.path(path_tables, "Comparacion_bases_propiedades.pdf")
png_path  <- file.path(path_tables, "Comparacion_bases_propiedades.png")

gtsave(tabla_comp_prop, html_path)
webshot2::webshot(html_path, pdf_path)
webshot2::webshot(html_path, png_path)

#Resumen tabla
tabla_comp_prop



# Tipos de datos y vistazo

cat("\nGlimpse train:\n")
glimpse(train)
cat("\nGlimpse test:\n")
glimpse(test)
#Pice vacio en test

# Resumen estad√≠stico b√°sico de train
cat("\nResumen SKIM (train):\n")
skimr::skim(train)

#6 variables caracter
#10 variables numericas
#Faltantes en m2 total, m2 cubierta, cuartos y ba√±os.

str(train)
```


## Revisamos duplicados 

```{r}

# Duplicados en ID
id_col <- dplyr::case_when(
  "property_id" %in% names(train) ~ "property_id",
  TRUE ~ NA_character_
)

if (!is.na(id_col)) {
  dup_train <- train %>% count(.data[[id_col]]) %>% filter(n > 1) %>% nrow()
  dup_test  <- test  %>% count(.data[[id_col]]) %>% filter(n > 1) %>% nrow()
  cat("\nDuplicados por ID en train:", dup_train, " | en test:", dup_test, "\n")
} else {
  cat("\n(No hay duplicados)\n")
  cat("Filas duplicadas en train:", sum(duplicated(train)), "\n")
  cat("Filas duplicadas en test :", sum(duplicated(test)),  "\n")
}

getwd()
```
#Revis√≥n de NA

```{r}


# Calcular % de faltantes en TRAIN hogares
faltantes_train <- train %>%
  summarise(across(everything(), ~mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Porcentaje_NA") %>%
  mutate(Base = "Entrenamiento")

# Calcular % de faltantes en TEST hogares
faltantes_test <- test %>%
  summarise(across(everything(), ~mean(is.na(.)) * 100)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Porcentaje_NA") %>%
  mutate(Base = "Prueba")

# Unir ambas bases
faltantes_comp <- bind_rows(faltantes_train, faltantes_test)


# Gr√°fico comparativo con leyenda inferior
grafico_faltantes <- ggplot(faltantes_comp, aes(x = Variable, y = Porcentaje_NA, fill = Base)) +
  geom_bar(stat = "identity", position = "dodge", color = "gray30") +
  geom_text(
    aes(label = paste0(round(Porcentaje_NA, 2), "%")),
    position = position_dodge(width = 0.9),
    vjust = -0.3, size = 3
  ) +
  coord_flip() +
  labs(
    title = "Comparaci√≥n de % de valores faltantes por variable\nTrain vs Test",
    x = "Variable",
    y = "% de valores faltantes",
    fill = "Base de datos"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom",              # üîπ Mueve la leyenda abajo
    legend.title = element_text(size = 11),
    legend.text = element_text(size = 10),
    axis.text.y = element_text(size = 9),
    axis.text.x = element_text(size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank()
  ) +
  scale_fill_manual(
    values = c("Entrenamiento" = "lightyellow4", "Prueba" = "#FFFF00")
  )



#Gardar Gr√°fica

root_dir <- here::here()

# Crea subrutas relativas al proyecto
path_outputs <- file.path(root_dir, "outputs")
path_figures <- file.path(path_outputs, "figures")

#PDF
ggsave(
  filename = file.path(path_figures, "Faltantes_Trains.pdf"),
  plot = grafico_faltantes,
  width = 10, height = 6
)

#PNG
ggsave(
  filename = file.path(path_figures, "Faltantes_Trains.png"),
  plot = grafico_faltantes,
  width = 10, height = 6,
  dpi = 300
)

# Mostrar en pantalla
print(grafico_faltantes)



```
## Revisi√≥n outliers

```{r}

# Seleccionar variables num√©rica
vars_outliers <- train %>%
  select(where(is.numeric)) %>%
  select(-any_of(c("price", "year", "lon", "lat", "month")))

#‚É£ Pasar a formato largo
vars_long <- vars_outliers %>%
  pivot_longer(cols = everything(),
               names_to = "Variable",
               values_to = "Valor")

#  Graficar boxplots 
grafico_outliers <- ggplot(vars_long, aes(x = Variable, y = Valor)) +
  geom_boxplot(fill = "#87CEFA", color = "gray30", outlier.colour = "red", alpha = 0.6, width = 0.6) +
  coord_flip() +
  labs(
    title = "Detecci√≥n de outliers",
    x = "Variable",
    y = "Valor"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
    axis.title.x = element_text(size = 12, margin = margin(t = 10)),
    axis.title.y = element_text(size = 12, margin = margin(r = 10)),
    axis.text.y = element_text(size = 11),
    axis.text.x = element_text(size = 10),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank()
  )


print(grafico_outliers)

#Gurdamos gr√°fica

# Ruta din√°mica
root_dir <- here::here()
path_outputs <- file.path(root_dir, "outputs")
path_figures <- file.path(path_outputs, "figures")
dir.create(path_figures, recursive = TRUE, showWarnings = FALSE)


#PDF
ggsave(
  filename = file.path(path_figures, "Outliers.pdf"),
  plot = grafico_outliers,
  width = 9, height = 12, dpi = 300
)
#PNG
ggsave(
  filename = file.path(path_figures, "Outliers.png"),
  plot = grafico_outliers,
  width = 9, height = 12, dpi = 300
)

#Revismos si hay observaciones donde el cubierto sea mayor que el total
train %>%
  filter(surface_covered > surface_total) %>%
  select(surface_total, surface_covered)
```

## Variables nuevas a partir de texto

```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(stringi)
library(purrr)


# Normalizador de texto

texto_normal <- function(x) {
  x %>%
    tidyr::replace_na("") %>%
    stringi::stri_trans_general("Latin-ASCII") %>%
    tolower() %>%
    stringr::str_squish()
}


#  Diccionario

token_diccionario <- list(
  parqueadero_garaje = c("parqueadero", "garaje", "estacionamiento"),
  seguridad          = c("vigilancia", "porter√≠a", "seguridad", "camaras", "guardian", "portero"),
  ascensor           = c("ascensor", "elevador"),
  gimnasio           = c("gimnasio", "gym"),
  piscina            = c("piscina", "pileta", "alberca"),
  bbq                = c("bbq", "asador", "barbacoa"),
  salon_social       = c("salon social", "zona social", "salon comunal"),
  zona_infantil      = c("zona infantil", "juegos", "parque infantil"),
  balcon             = c("balcon", "balc√≥n"),
  terraza            = c("terraza", "roof", "azotea"),
  patio              = c("patio", "patio interior"),
  jardin_exterior    = c("jardin", "jard√≠n", "zona verde"),
  chimenea           = c("chimenea", "hogar", "chimeneas"),
  cocina_integral    = c("cocina integral", "cocina moderna"),
  deposito           = c("deposito", "baulera", "trastero", "bodega"),
  estudio            = c("estudio", "oficina", "biblioteca"),
  remodelado         = c("remodelado", "renovado", "actualizado", "reformado"),
  vista              = c("vista", "panoramica", "panor√°mica", "mirador"),
  pisos_vivienda     = c("dos pisos", "tres pisos", "cuatro pisos", "2 pisos", "3 pisos")
)


# Funci√≥n robusta

add_text_dummies <- function(df, dicc = token_diccionario) {
  cat("\n>>> Iniciando funci√≥n add_text_dummies()\n")

  # Asegurar tipo base data.frame
  df <- as.data.frame(df)

  # Crear ID manual
  df$id_tmp <- seq_len(nrow(df))

  # Unificar y normalizar texto
  df$texto_total <- texto_normal(paste(df$title, df$description))

  # Guardar una copia con solo texto e id
  tokens_df <- data.frame(
    id_tmp = df$id_tmp,
    texto_total = df$texto_total,
    stringsAsFactors = FALSE
  )

 
  # tokenizamos sin tibble ni spec_tbl_df
  tokens <- tidytext::unnest_tokens(tokens_df, token, texto_total, to_lower = TRUE)


  # Crear las dummies por categor√≠a
  features_list <- list()
  for (cat in names(dicc)) {
    palabras <- dicc[[cat]]
    hits <- tokens %>%
      dplyr::filter(token %in% palabras) %>%
      dplyr::distinct(id_tmp) %>%
      dplyr::mutate(value = 1L)

    base <- tibble(id_tmp = df$id_tmp)
    merged <- base %>%
      dplyr::left_join(hits, by = "id_tmp") %>%
      dplyr::mutate(value = tidyr::replace_na(value, 0L)) %>%
      dplyr::rename(!!paste0("has_", cat) := value)

    features_list[[cat]] <- merged
  }

  # Combinar las dummies por id_tmp
  features <- purrr::reduce(features_list, dplyr::left_join, by = "id_tmp")

  # Unir con el df original
  out <- dplyr::left_join(df, features, by = "id_tmp") %>%
    dplyr::select(-texto_total, -id_tmp)

 
  return(out)
}

# ============================================================
#. Aplicar a train y test

train_txt <- add_text_dummies(train)
test_txt  <- add_text_dummies(test)


# Verificar cobertura

colMeans(dplyr::select(train_txt, starts_with("has_")), na.rm = TRUE)
colMeans(dplyr::select(test_txt,  starts_with("has_")), na.rm = TRUE)




```




# Variables Internas (4 variables que salen de la columna de texto en ambas bases de datos)

```{r}



# Funci√≥n para evaluar variables binarias vs. precio

score_one_attr <- function(df, attr_col, target = "price") {
  g <- df %>%
    summarise(
      mean1 = mean(.data[[target]][.data[[attr_col]] == 1], na.rm = TRUE),
      mean0 = mean(.data[[target]][.data[[attr_col]] == 0], na.rm = TRUE),
      n1    = sum(.data[[attr_col]] == 1, na.rm = TRUE),
      n0    = sum(.data[[attr_col]] == 0, na.rm = TRUE),
      .groups = "drop"
    )

  effect   <- g$mean1 - g$mean0
  pct_diff <- ifelse(is.finite(g$mean0) && g$mean0 != 0, effect / g$mean0, NA_real_)
  grand    <- mean(df[[target]], na.rm = TRUE)
  ss_between <- g$n1 * (g$mean1 - grand)^2 + g$n0 * (g$mean0 - grand)^2
  ss_total   <- sum((df[[target]] - grand)^2, na.rm = TRUE)
  eta2 <- ifelse(ss_total > 0, ss_between / ss_total, 0)

  tibble(
    variable = attr_col,
    support_1 = g$n1,
    rate_1 = g$n1 / (g$n1 + g$n0),
    mean_price_1 = g$mean1,
    mean_price_0 = g$mean0,
    mean_diff = effect,
    pct_diff = pct_diff,
    eta2 = eta2
  )
}

# Separar por tipo de propiedad

train_apto <- train_txt %>% filter(property_type %in% c("Apartamento"))
train_casa <- train_txt %>% filter(property_type %in% c("Casa"))


#  Seleccionar variables dummies

flag_cols <- names(train_txt)[grepl("^has_", names(train_txt))]


#  Evaluar cada dummy por grupo

stats_apto <- map_dfr(flag_cols, ~score_one_attr(train_apto, .x, target = "price")) %>%
  mutate(tipo = "Apartamento")
stats_casa <- map_dfr(flag_cols, ~score_one_attr(train_casa, .x, target = "price")) %>%
  mutate(tipo = "Casa")

# Combinar resultados
stats_all <- bind_rows(stats_apto, stats_casa)


#  Ranking y selecci√≥n de las mejores

ranked <- stats_all %>%
  filter(rate_1 >= 0.03) %>% # Al menos 3% de cobertura
  mutate(rank_score = eta2 * 0.5 + abs(pct_diff) * 0.3 + rate_1 * 0.2) %>%
  arrange(tipo, desc(rank_score))

# Top 5 por tipo
top_by_type <- ranked %>%
  group_by(tipo) %>%
  slice_max(order_by = rank_score, n = 5) %>%
  ungroup() %>%
  mutate(
    pct_diff = percent(pct_diff, accuracy = 0.1),
    rate_1 = percent(rate_1, accuracy = 0.1)
  ) %>%
  select(tipo, variable, rate_1, mean_price_1, mean_price_0, pct_diff, eta2)


# 5 Resultado

print(top_by_type, n = Inf)


# Variables finales seleccionadas
best_vars <- unique(top_by_type$variable)
print(best_vars)


#  Crear datasets finales coherentes

add_best_vars <- function(df, vars) {
  # Asegura que las columnas existan en test
  for (nm in vars) if (!nm %in% names(df)) df[[nm]] <- 0L
  df %>% mutate(across(all_of(vars), ~as.integer(.)))
}

train_ready <- add_best_vars(train_txt, best_vars)
test_ready  <- add_best_vars(test_txt,  best_vars)


```

# Obtenci√≥n de variables a partir de otras fuentes

### Variables de Open Street Map

1. Carga datos de viviendas (train y test)
2. Usa informaci√≥n de OpenStreetMap para:
   - Asociar barrio y localidad
   - Calcular distancia a parques, colegios, restaurantes, autopistas y centros comerciales
3. Si los datos procesados ya existen en formato .rds, los carga directamente
(para evitar volver a correr la carga pesada del archivo OSM .pbf)


```{r}

# CONFIGURACI√ìN GENERAL

options(timeout = 1800)
Sys.setenv(OGR_GEOMETRY_ACCEPT_UNCLOSED_RING = "YES")
sf_use_s2(TRUE)

library(sf)
library(dplyr)
library(purrr)
library(stringr)
library(units)
library(here)
library(osmextract)

# RUTA Y √ÅREA DE INTER√âS

# PBF (Protocolbuffer Binary Format) es el formato comprimido de OSM - OpenStreetMap.
file_pbf <- here::here("data", "external", "colombia-251111.osm.pbf")

# delimitar area geografica a Bogot√°
bbox_bogota <- st_bbox(c(
  xmin = -74.35, xmax = -73.9,
  ymin = 4.45,  ymax = 4.85
), crs = 4326)

<<<<<<< HEAD
#  LECTURA SEGURA DE CAPAS OSM - OpenStreetMap.
=======



#  LECTURA SEGURA DE CAPAS OSM
>>>>>>> c256b2b01fbbf419d4eb9549096bc3755d8a131e

extract_tag <- function(df, key) {
  if (!is.null(df) && !"other_tags" %in% names(df)) return(df)
  if (key %in% names(df)) return(df)
  if (!is.null(df$other_tags)) {
    df[[key]] <- stringr::str_match(df$other_tags, paste0('"', key, '"=>"(.*?)"'))[, 2]
  }
  df
}

# Aplicar extracci√≥n a todas las capas antes del filtrado
#leisure: lugares de recreaci√≥n y entretenimiento.
points <- extract_tag(points, "leisure") #Puntos de interes con atributos
polys  <- extract_tag(polys,  "leisure") #Puntos de interes que son areas
lines  <- extract_tag(lines,  "leisure") #Puntos de interes lineales

#landuse: uso del suelo
points <- extract_tag(points, "landuse")
polys  <- extract_tag(polys,  "landuse")
lines  <- extract_tag(lines,  "landuse")

#amenity: Servicios y equipamientos disponibles para el p√∫blico.
points <- extract_tag(points, "amenity")
polys  <- extract_tag(polys,  "amenity")
lines  <- extract_tag(lines,  "amenity")

<<<<<<< HEAD
# Funci√≥n general
=======


#EXTRAER BARRIOS Y LOCALIDADES
localidades <- polys %>%
  filter(admin_level == "8") %>%
  select(name) %>%
  rename(localidad = name)

barrios <- polys %>%
  filter(admin_level == "10" | place == "neighbourhood") %>%
  select(name) %>%
  rename(barrio = name)


# Funci√≥n general (igual que antes)
>>>>>>> c256b2b01fbbf419d4eb9549096bc3755d8a131e
get_poi_all <- function(points, polys, lines, key, values) {
  capas <- list(points, polys, lines)
  pois <- purrr::map_dfr(capas, function(x) {
    if (is.null(x) || !key %in% names(x)) return(NULL)
    x %>% filter(.data[[key]] %in% values)
  })
  if (nrow(pois) == 0) return(NULL)
  pois
}

# Crear lista de POIs con criterios extendidos ---
pois_list <- list(
  parques = bind_rows(
    get_poi_all(points, polys, lines, "leisure",
                c("park", "garden", "recreation_ground", "playground", "golf_course")),
    get_poi_all(points, polys, lines, "landuse", c("recreation_ground", "grass", "greenfield")),
    get_poi_all(points, polys, lines, "amenity", c("park"))
  ),
  colegios = get_poi_all(points, polys, lines, "amenity",
                         c("school", "kindergarten", "college", "university")),
  restaurantes = get_poi_all(points, polys, lines, "amenity",
                             c("restaurant", "fast_food", "cafe", "bar", "pub")),
  autopistas = get_poi_all(points, polys, lines, "highway",
                           c("motorway", "trunk", "primary", "secondary", "tertiary")),
  centros_comerciales = get_poi_all(points, polys, lines, "shop",
                                    c("mall", "supermarket", "marketplace"))
)

# Limpiar vac√≠os
pois_list <- purrr::discard(pois_list, ~is.null(.) || nrow(.) == 0)


#  CONVERTIR A SF Y TRANSFORMAR CRS A 3116 (m√©trico)
train_sf <- st_as_sf(train_ready, coords = c("lon", "lat"), crs = 4326, remove = FALSE)
test_sf  <- st_as_sf(test_ready,  coords = c("lon", "lat"), crs = 4326, remove = FALSE)

train_sf <- st_transform(train_sf, 3116)
test_sf  <- st_transform(test_sf, 3116)
localidades <- st_transform(localidades, 3116)
barrios     <- st_transform(barrios, 3116)
pois_sf     <- map(pois_list, ~st_transform(., 3116))


#  FUNCI√ìN DE DISTANCIAS

calc_min_dist <- function(x, y) {
  if (is.null(y) || nrow(y) == 0) return(rep(NA, nrow(x)))
  st_distance(x, y) %>%
    apply(1, min) %>%
    set_units("m") %>%
    drop_units()
}

for (nm in names(pois_sf)) {
  message("üìè Calculando distancia a ", nm, " ...")
  train_sf[[paste0("dist_", nm)]] <- calc_min_dist(train_sf, pois_sf[[nm]])
  test_sf [[paste0("dist_", nm)]] <- calc_min_dist(test_sf , pois_sf[[nm]])
}

# UNI√ìN CON LOCALIDADES Y BARRIOS

train_sf <- train_sf %>%
  st_join(barrios, join = st_within) %>%
  st_join(localidades, join = st_within)

test_sf <- test_sf %>%
  st_join(barrios, join = st_within) %>%
  st_join(localidades, join = st_within)

# REASIGNAR CRS Y GUARDAR

st_crs(train_sf) <- NA
st_crs(test_sf)  <- NA

st_crs(train_sf) <- 4326
st_crs(test_sf)  <- 4326

train_ready_osm <- st_drop_geometry(train_sf)
test_ready_osm  <- st_drop_geometry(test_sf)

saveRDS(train_ready_osm, "outputs/train_ready_osm.rds")
saveRDS(test_ready_osm,  "outputs/test_ready_osm.rds")
saveRDS(polys, "outputs/polys.rds")
saveRDS(lines,  "outputs/lines.rds")
saveRDS(points, "outputs/points.rds")
saveRDS(barrios, "outputs/barrios.rds")
saveRDS(localidades,  "outputs/localidades.rds")
saveRDS(pois_list, "outputs/pois_list.rds")

```

## Gr√°fica: Viviendas en venta del test y train

En esta secci√≥n se gr√°fica las viviendas obtenidas en la base de datos de entrenamiento y de testeo visualizando concentraciones de vivienda.

```{r}

#  Asegurar sf + CRS
to_sf_4326 <- function(df) {
  if ("sf" %in% class(df)) return(st_transform(df, 4326))
  st_as_sf(df, coords = c("lon","lat"), crs = 4326, remove = FALSE)
}

train_sf <- to_sf_4326(train_ready_osm)
test_sf  <- to_sf_4326(test_ready_osm)

train_sf$dataset <- "Train"
test_sf$dataset  <- "Test"
viviendas_sf     <- bind_rows(train_sf, test_sf)

# Foondos de localidades y barrios
bg_layers <- list()

if (exists("localidades") && inherits(localidades, "sf") && nrow(localidades) > 0) {
  bg_layers$localidades <- st_transform(localidades, 4326)
}
if (exists("barrios") && inherits(barrios, "sf") && nrow(barrios) > 0) {
  bg_layers$barrios <- st_transform(barrios, 4326)
}

# Extensi√≥n del mapa: 
bb   <- st_bbox(viviendas_sf)
padx <- diff(range(c(bb["xmin"], bb["xmax"]))) * 0.05  # 5% de margen
pady <- diff(range(c(bb["ymin"], bb["ymax"]))) * 0.05

xlim <- c(bb["xmin"] - padx, bb["xmax"] + padx)
ylim <- c(bb["ymin"] - pady, bb["ymax"] + pady)

# Mapa en ggplot2
m  <- ggplot()

# Fondo: localidades
if (!is.null(bg_layers$localidades)) {
m <- m + geom_sf(data = bg_layers$localidades, fill = NA, color = "grey70", linewidth = 0.3)
}

# Fondo: barrios 
if (!is.null(bg_layers$barrios)) {
  m <- m + geom_sf(data = bg_layers$barrios, fill = NA, color = "grey85", linewidth = 0.2)
}

# Puntos de viviendas
m <- m +
  geom_sf(data = viviendas_sf,
          aes(color = dataset),
          size = 0.35, alpha = 0.7, show.legend = TRUE) +
  scale_color_manual(values = c("Train" = "#1E88E5", "Test" = "#E53935"),
                     name = "Conjunto de datos") +
  coord_sf(xlim = xlim, ylim = ylim, expand = FALSE) +
  theme_minimal(base_size = 11) +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 9),
    panel.grid = element_blank(),
    axis.text.x = element_text(size = 8),   # ejes m√°s peque√±os
    axis.text.y = element_text(size = 8),
    plot.title  = element_text(face = "bold", hjust = 0.5)
  ) +
  labs(
    title = "Viviendas en Bogot√°",
    x = "Longitud", y = "Latitud"
  )

m

#Directorio
root_dir <- here::here()

# Crea subrutas relativas al proyecto
path_outputs <- file.path(root_dir, "outputs")
path_figures <- file.path(path_outputs, "figures")

#  Guardar
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
ggsave("outputs/figures/mapa_train_test_bogota.png", m , width = 8, height = 6, dpi = 300)

```

## Mapa de viviendas y puntos de inter√©s


```{r}
# Asegurar que todos los objetos est√©n en CRS 3116
train_sf <- st_transform(train_sf, 3116)
pois_sf  <- map(pois_sf, ~st_transform(., 3116))
barrios  <- st_transform(barrios, 3116)
localidades <- st_transform(localidades, 3116)

# Convertir pol√≠gonos a centroides (si existen)
pois_sf <- map(pois_sf, function(x) {
  if (any(grepl("POLYGON", unique(st_geometry_type(x))))) {
    x <- st_centroid(x)
  }
  x
})

# Combinar todos los POIs en una sola capa con una columna "tipo"
pois_union <- map2_dfr(pois_sf, names(pois_sf), ~mutate(.x, tipo = .y))

# Asegurar que "tipo" sea un factor ordenado con nombres legibles
pois_union <- pois_union %>%
  mutate(tipo = factor(
    tipo,
    levels = c("parques", "colegios", "restaurantes", "autopistas", "centros_comerciales"),
    labels = c("Parques", "Colegios", "Restaurantes", "Autopistas", "Centros comerciales")
  ))

# Colores para los distintos puntos de inter√©s
colores_poi <- c(
  "Parques" = "#4CAF50",
  "Colegios" = "#FFD600",
  "Restaurantes" = "#E64A19",
  "Autopistas" = "#1565C0",
  "Centros comerciales" = "#9C27B0"
)

# Crear bounding box extendido a partir de las viviendas
bb <- st_bbox(train_sf)
padx <- (bb["xmax"] - bb["xmin"]) * 0.05
pady <- (bb["ymax"] - bb["ymin"]) * 0.05
xlim <- c(bb["xmin"] - padx, bb["xmax"] + padx)
ylim <- c(bb["ymin"] - pady, bb["ymax"] + pady)

# Graficar mapa
m_poi<-ggplot() +
  # Localidades de fondo
  geom_sf(data = localidades, fill = "gray95", color = "gray85", size = 0.2) +
  
  # Barrios
  geom_sf(data = barrios, fill = NA, color = "gray90", size = 0.1) +
  
  # Viviendas (Train)
  geom_sf(data = train_sf, color = "black", size = 0.25, alpha = 0.5) +

  # Puntos de inter√©s
  geom_sf(data = pois_union, aes(color = tipo), size = 0.7, alpha = 0.7, show.legend = TRUE) +

  # Escala de colores y leyenda corregida
  scale_color_manual(
    values = colores_poi,
    name = "Punto de inter√©s"
  ) +

  # L√≠mites espaciales
  coord_sf(xlim = xlim, ylim = ylim, expand = FALSE) +

  # T√≠tulos y etiquetas
  labs(
    title = "Viviendas y Puntos de Inter√©s",
    subtitle = "Fuentes: OpenStreetMap y GeoFabrik Colombia",
    x = "Coordenada Este (metros)", 
    y = "Coordenada Norte (metros)"
  ) +

  # Estilo gr√°fico
  theme_minimal(base_size = 11) +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9),
    panel.grid = element_blank(),
    axis.text.x = element_text(size = 8),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )

#  Guardar

root_dir <- here::here()

# Crea subrutas relativas al proyecto
path_outputs <- file.path(root_dir, "outputs")
path_figures <- file.path(path_outputs, "figures")


dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
ggsave("outputs/figures/mapa_train_pois_bogota.png", m_poi , width = 8, height = 6, dpi = 300)


m_poi
```


##Inclusi√≥n de estrato socioeconomico

A continuaci√≥n se asigna el estrato socioecon√≥mico a cada vivienda en las bases train y test a partir de los pol√≠gonos de lotes oficiales del DANE.

El procedimiento realiza los siguientes pasos:

- Carga de datos: se leen los archivos geoespaciales LOTE.gpkg (geometr√≠a) y estrato.csv (informaci√≥n tabular).

- Uni√≥n por c√≥digo del lote: se vincula cada lote con su estrato correspondiente y se limpia la informaci√≥n.

- Conversi√≥n a objetos espaciales (sf): las bases de viviendas (train y test) se transforman a coordenadas EPSG:3116 para asegurar compatibilidad geogr√°fica.

- Asignaci√≥n directa: se realiza una uni√≥n espacial (st_join) para asignar el estrato cuando la vivienda cae dentro de un lote.

- Imputaci√≥n por proximidad: para viviendas sin estrato (NA o 0), se usa el algoritmo k-nearest neighbor (RANN::nn2), que asigna el estrato del lote m√°s cercano, evitando alto costo computacional.

- Exportaci√≥n: los resultados finales se guardan en formato .rds y .csv para uso posterior.


```{r}

# Verificar si existen archivos procesados

train_rds_estrato <- "outputs/train_ready_osm_estrato.rds"
test_rds_estrato  <- "outputs/test_ready_osm_estrato.rds"
lines <- "outputs/lines.rds"
polys <- "outputs/polys.rds"
points<- "outputs/points.rds"


if (file.exists(train_rds_estrato) & file.exists(test_rds_estrato)) {

  train_ready_osm_estrato <- readRDS(train_rds_estrato)
  test_ready_osm_estrato  <- readRDS(test_rds_estrato)
  points <- readRDS(points)
  polys  <- readRDS(polys)
  lines <- readRDS(lines)

} else {


# Cargar fuentes externas 

  ruta_gpkg <- here("data", "external", "LOTE.gpkg")
  ruta_estrato_csv <- here("data", "external", "estrato.csv")

  if (!file.exists(ruta_gpkg)) stop("No se encontr√≥ el archivo 'LOTE.gpkg' en: ", ruta_gpkg)
  if (!file.exists(ruta_estrato_csv)) stop("No se encontr√≥ el archivo 'estrato.csv' en: ", ruta_estrato_csv)

  lotes_sf <- sf::st_read(ruta_gpkg, quiet = TRUE)
  estrato_df <- readr::read_csv(ruta_estrato_csv, show_col_types = FALSE)
 


# Preparar y unir Lotes + Estrato
  lotes_pre <- lotes_sf %>%
    rename(codigo_lote_llave = LOTCODIGO) %>%
    mutate(codigo_lote_llave = as.character(codigo_lote_llave))

  estrato_pre <- estrato_df %>%
    rename(codigo_lote_llave = ESOCLOTE) %>%
    mutate(codigo_lote_llave = as.character(codigo_lote_llave)) %>%
    distinct(codigo_lote_llave, .keep_all = TRUE)

  lotes_estrato_sf <- lotes_pre %>%
    left_join(estrato_pre, by = "codigo_lote_llave") %>%
    select(codigo_lote_llave, estrato = ESOESTRATO, geom) %>%
    filter(!is.na(estrato) & estrato > 0) %>%
    mutate(estrato = as.numeric(as.character(estrato))) %>%
    st_make_valid()


# Preparar viviendas (train/test)

  to_sf <- function(df) {
    if (!inherits(df, "sf"))
      st_as_sf(df, coords = c("lon", "lat"), crs = 4326, remove = FALSE)
    else df
  }

  train_sf <- to_sf(train_ready_osm) |> st_transform(3116)
  test_sf  <- to_sf(test_ready_osm)  |> st_transform(3116)
  lotes_estrato_sf <- st_transform(lotes_estrato_sf, 3116)


#  Uni√≥n espacial directa

  train_sf_join <- st_join(train_sf, lotes_estrato_sf, join = st_intersects, left = TRUE)
  test_sf_join  <- st_join(test_sf,  lotes_estrato_sf, join = st_intersects, left = TRUE)

# Normalizar el nombre y tipo de columna
  normalizar_estrato <- function(df) {
    df %>%
      rename(estrato = matches("estrato", ignore.case = TRUE)) %>%
      mutate(estrato = as.numeric(as.character(estrato)))
  }

  train_sf_join <- normalizar_estrato(train_sf_join)
  test_sf_join  <- normalizar_estrato(test_sf_join)


# Imputaci√≥n por proximidad (solo faltantes reales)


# Crear centroides de los lotes para representar cada pol√≠gono
lotes_centroides <- sf::st_centroid(lotes_estrato_sf)
coords_lotes <- sf::st_coordinates(lotes_centroides)

# Preparar los datos de entrenamiento 
coords_train <- sf::st_coordinates(train_sf_join)

# Identificar las viviendas sin estrato asignado
faltantes <- which(is.na(train_sf_join$estrato) | train_sf_join$estrato == 0)


if (length(faltantes) > 0) {
# Buscar el vecino m√°s cercano en los centroides de lotes
  nn <- RANN::nn2(
    data = coords_lotes, 
    query = coords_train[faltantes, ], #faltantes de estrato
    k = 1 #El mas cercano
  )
  
  # Asignar el estrato del lote m√°s cercano
  train_sf_join$estrato[faltantes] <- lotes_estrato_sf$estrato[nn$nn.idx]
} #indice del lote mas cercano

# Repetir para el test
coords_test <- sf::st_coordinates(test_sf_join)
faltantes_t <- which(is.na(test_sf_join$estrato) | test_sf_join$estrato == 0)

if (length(faltantes_t) > 0) {
  nn_t <- RANN::nn2(
    data = coords_lotes, 
    query = coords_test[faltantes_t, ], 
    k = 1
  )
  
  test_sf_join$estrato[faltantes_t] <- lotes_estrato_sf$estrato[nn_t$nn.idx]
}

#resultado despues de la imputaci√≥n y el join
train_sf_final <- train_sf_join
test_sf_final  <- test_sf_join


# Exportar resultados en rds y csv (por si acaso)

  train_ready_osm_estrato <- st_drop_geometry(train_sf_final)
  test_ready_osm_estrato  <- st_drop_geometry(test_sf_final)

  saveRDS(train_ready_osm_estrato, train_rds_estrato)
  saveRDS(test_ready_osm_estrato,  test_rds_estrato)

  readr::write_csv(train_ready_osm_estrato, "outputs/train_final_con_estrato.csv")
  readr::write_csv(test_ready_osm_estrato,  "outputs/test_final_con_estrato.csv")

}

```

#### Correcciones
La capa de parques se encontraba da√±ada por ende, en el siguiente bloque de codigo se repara:

```{r}



# Funci√≥n que extrae tags desde other_tags (si no existen)

extract_tag <- function(df, key) {
  if (key %in% names(df)) return(df)
  if (!"other_tags" %in% names(df)) return(df)
  df[[key]] <- str_match(df$other_tags, paste0('"', key, '"=>"(.*?)"'))[, 2]
  df
}


#  Extraer leisure, landuse, amenity SOLO para parques

tags_needed <- c("leisure", "landuse", "amenity")

for (tg in tags_needed) {
  points_bogota <- extract_tag(points, tg)
  polys_bogota  <- extract_tag(polys,  tg)
  lines_bogota  <- extract_tag(lines,  tg)
}


# Reconstruir parques desde leisure / landuse / amenity



parques <- bind_rows(

  # LEISURE
  points_bogota %>% filter(leisure %in% c("park", "garden", "recreation_ground", "playground")),
  polys_bogota  %>% filter(leisure %in% c("park", "garden", "recreation_ground", "playground")),
  lines_bogota  %>% filter(leisure %in% c("park", "garden", "recreation_ground", "playground")),

  # LANDUSE
  points_bogota %>% filter(landuse %in% c("recreation_ground", "grass")),
  polys_bogota  %>% filter(landuse %in% c("recreation_ground", "grass")),
  lines_bogota  %>% filter(landuse %in% c("recreation_ground", "grass")),

  # AMENITY
  points_bogota %>% filter(amenity == "park"),
  polys_bogota  %>% filter(amenity == "park"),
  lines_bogota  %>% filter(amenity == "park")
)

# Quitar duplicados
parques <- parques %>% distinct(osm_id, .keep_all = TRUE)


#  parques en boundary = protected_area

if ("boundary" %in% names(polys_bogota)) {
  parques_pa <- polys_bogota %>% filter(boundary %in% c("protected_area", "national_park"))
  if (nrow(parques_pa) > 0) {
    cat("   + Parques adicionales en protected_area: ", nrow(parques_pa), "\n")
    parques <- bind_rows(parques, parques_pa) %>%
      distinct(osm_id, .keep_all = TRUE)
  }
}

cat("   Total parques finales: ", nrow(parques), "\n\n")

# Convertir parques a sf v√°lido y transformar a 3116

if (nrow(parques) == 0) {
  stop(" no se detectaron parques")
}



parques_sf <- parques %>%
  st_make_valid() %>%
  st_transform(3116)

# Si hay l√≠neas ‚Üí buffer
if (any(st_geometry_type(parques_sf) %in% c("LINESTRING", "MULTILINESTRING"))) {
  parques_sf <- parques_sf %>%
    st_cast("MULTILINESTRING") %>%
    st_buffer(20)
}

parques_sf <- parques_sf[!st_is_empty(parques_sf), ]

# Funci√≥n de distancia

calc_min_dist <- function(x, y) {
  if (is.null(y) || nrow(y) == 0) return(rep(NA_real_, nrow(x)))
  x <- st_make_valid(x)
  y <- st_make_valid(y)
  y <- y[!st_is_empty(y), ]
  if (nrow(y) == 0) return(rep(NA_real_, nrow(x)))

  d <- suppressWarnings(st_distance(x, y))

  apply(d, 1, function(v) {
    if (all(is.na(v))) NA else min(v, na.rm = TRUE)
  }) %>% set_units("m") %>% drop_units()
}


#  Calcular SOLO dist_parques

train_ready_osm_estrato <- st_as_sf(train_ready_osm_estrato, coords = c("lon", "lat"), crs = 4326)
test_ready_osm_estrato <- st_as_sf(test_ready_osm_estrato,  coords = c("lon", "lat"), crs = 4326)

# Transformar a m√©trico (3116)
train_ready_osm_estrato <- st_transform(train_ready_osm_estrato, 3116)
test_ready_osm_estrato <- st_transform(test_ready_osm_estrato, 3116)

#Calcular distancias

train_ready_osm_estrato$dist_parques <- calc_min_dist(train_ready_osm_estrato, parques_sf)


test_ready_osm_estrato$dist_parques  <- calc_min_dist(test_ready_osm_estrato, parques_sf)


print(summary(train_ready_osm_estrato$dist_parques))

```


## Manipulaci√≥n final de datos

Una vez revisado problematicas de valores faltantes y outlier y correlaci√≥n espacial. Se procede a corregir las bases para evitar el sobre ajuste y la falta de variables altamente significativas.

### Autocrrelaci√≥n espacial.

En mapas anteriores, se puede ver que en la base de entrenamiento, cuenta con viviendas ubicadas en chapinero, o muy cercanas a esta localidad lo cual puede incurrir en que el modelo se aprenda ubicaciones m√°s no generalice, causando error fuera de muestra. No obstante, para corregir dicho error, se estima un buffer de 1km  seg√∫n los datos de la base de prueba que permita que el modelo pueda generalizar mejor 

```{r}


# Convertir a sf

train_buffer <- st_as_sf(train_ready_osm_estrato, coords = c("lon", "lat"), crs = 4326)
test_buffer  <- st_as_sf(test_ready_osm_estrato,  coords = c("lon", "lat"), crs = 4326)

# Transformar a proyecci√≥n m√©trica (en metros)
train_buffer <- st_transform(train_buffer, 3116)
test_buffer <- st_transform(test_buffer, 3116)

chapinero_test <- test_buffer |> filter(localidad == "Localidad Chapinero")


# Crear un buffer de 500 mts alrededor de todas las viviendas de Chapinero
buffer_chapinero <- chapinero_test |>
  st_union() |>          
  st_buffer(dist = 500) 

# Identificar cu√°les puntos del train est√°n dentro del buffer
idx_intersects <- st_intersects(train_buffer, buffer_chapinero, sparse = FALSE)[,1]

# Excluir esos puntos del train
train_sf_clean <- train_buffer[!idx_intersects, ]


# Visualizaci√≥n
plot(st_geometry(train_buffer), col = "grey80")
plot(st_geometry(buffer_chapinero), add = TRUE, border = "red", lwd = 2)






```
### Manipulaci√≥n valores faltantes 

```{r}

#Primero intentamos extraer informaci√≥n de la descripcion 
extract_from_text <- function(text) {
  
  txt <- tolower(text)
  
  # ba√±os
  ba√±os <- str_extract(txt, "(\\d+)\\s*(ba√±os|banos|ba√±o|bano)")
  ba√±os <- as.numeric(str_extract(ba√±os, "\\d+"))
  
  # habitaciones
  habs <- str_extract(txt, "(\\d+)\\s*(habitaciones|habitacion|cuartos|cuarto|alcobas|alcoba)")
  habs <- as.numeric(str_extract(habs, "\\d+"))
  
  # √°rea total
  area <- str_extract(txt, "(\\d+)\\s*(m2|metros|mts)")
  area <- as.numeric(str_extract(area, "\\d+"))
  
  tibble(
    text_bathrooms  = ba√±os,
    text_bedrooms   = habs,
    text_surface    = area
  )
}

train_text_features <- extract_from_text(train_sf_clean$description)
test_text_features  <- extract_from_text(test_ready_osm_estrato$description)

# A√±adir estas variables

train_sf_clean <- bind_cols(train_sf_clean, train_text_features)
test_ready_osm_estrato <- bind_cols(test_ready_osm_estrato, test_text_features)


fill_from_text <- function(df) {
  df %>%
    mutate(
      bathrooms = ifelse(is.na(bathrooms) & !is.na(text_bathrooms), text_bathrooms, bathrooms),
      bedrooms  = ifelse(is.na(bedrooms)  & !is.na(text_bedrooms),  text_bedrooms,  bedrooms),
      surface_total = ifelse(is.na(surface_total) & !is.na(text_surface), text_surface, surface_total)
    )
}

train_sf_clean <- fill_from_text(train_sf_clean)
test_ready_osm_estrato <- fill_from_text(test_ready_osm_estrato)


# Extraer variables a imputar (sin geometr√≠a)

train_imp <- train_sf_clean %>%
  st_drop_geometry() %>%
  select(surface_total, surface_covered, bedrooms, bathrooms, property_type)

test_imp <- test_ready_osm_estrato %>%
  st_drop_geometry() %>%
  select(surface_total, surface_covered, bedrooms, bathrooms, property_type)


# Unir datos para imputaci√≥n consistente


full <- bind_rows(
  train_imp %>% mutate(dataset = "train"),
  test_imp  %>% mutate(dataset = "test")
)


# Funci√≥n general de imputaci√≥n


impute_var <- function(data, target, predictors) {
  
  df <- data
  
  complete_rows <- df %>% drop_na(all_of(c(target, predictors)))
  missing_rows  <- df %>% filter(is.na(.data[[target]]))
  
  if (nrow(missing_rows) == 0) return(df)
  
  rec <- recipe(as.formula(
    paste(target, "~", paste(predictors, collapse = "+"))
  ), data = complete_rows)
  
  mod <- rand_forest(mtry = 2, trees = 300, min_n = 5) %>%
    set_engine("ranger") %>%
    set_mode("regression")
  
  wf <- workflow() %>% add_recipe(rec) %>% add_model(mod)
  
  fit_mod <- fit(wf, complete_rows)
  
  preds <- predict(fit_mod, missing_rows) %>% pull(.pred)
  
  df[[target]][is.na(df[[target]])] <- preds
  
  df
}


# Imputaci√≥n encadenada


# bedrooms
full <- impute_var(full, "bedrooms",
                   predictors = c("bathrooms", "surface_total", "surface_covered", "property_type"))

# bathrooms
full <- impute_var(full, "bathrooms",
                   predictors = c("bedrooms", "surface_total", "surface_covered", "property_type"))

# surface_covered
full <- impute_var(full, "surface_covered",
                   predictors = c("bedrooms", "bathrooms", "surface_total", "property_type"))

# surface_total
full <- impute_var(full, "surface_total",
                   predictors = c("bedrooms", "bathrooms", "surface_covered", "property_type"))


# Separar nuevamente train / test


train_imputed <- full %>%
  filter(dataset == "train") %>%
  select(-dataset)

test_imputed <- full %>%
  filter(dataset == "test") %>%
  select(-dataset)


# Insertar imputaciones en bases originales


train_final <- train_sf_clean %>%
  mutate(
    surface_total   = train_imputed$surface_total,
    surface_covered = train_imputed$surface_covered,
    bedrooms        = train_imputed$bedrooms,
    bathrooms       = train_imputed$bathrooms
  )

test_final <- test_ready_osm_estrato %>%
  mutate(
    surface_total   = test_imputed$surface_total,
    surface_covered = test_imputed$surface_covered,
    bedrooms        = test_imputed$bedrooms,
    bathrooms       = test_imputed$bathrooms
  )


```

### Revisi√≥n final de datos y eliminaci√≥n de variables no necesarias

```{r}
#Miramos que variables tenemos
colnames(train_final)
#Eliminamos variables innecesarias
cols_to_drop <- c(
  "codigo_lote_llave",
  "text_bathrooms",
  "text_bedrooms",
  "text_surface"
)

train_final <- train_final %>% select(-any_of(cols_to_drop))
test_final <- test_final%>% select(-any_of(cols_to_drop))


# Revisamos estructura de datos que coincida con lo que es: 
str(train_final)

#
train_final$property_type <- as.factor(train_final$property_type)
test_final$property_type <- as.factor(test_final$property_type)

# 2. Lista de dummies
dummy_vars <- c(
  "has_parqueadero_garaje",
  "has_seguridad",
  "has_ascensor",
  "has_gimnasio",
  "has_piscina",
  "has_bbq",
  "has_salon_social",
  "has_zona_infantil",
  "has_balcon",
  "has_terraza",
  "has_patio",
  "has_jardin_exterior",
  "has_chimenea",
  "has_cocina_integral",
  "has_deposito",
  "has_estudio",
  "has_remodelado",
  "has_vista",
  "has_pisos_vivienda"
)

# Convertir dummies a factor
train_final <- train_final %>% mutate(across(all_of(dummy_vars), ~as.factor(.)))
test_final <- test_final %>% mutate(across(all_of(dummy_vars), ~as.factor(.)))

# Redondear en train
train_final <- train_final %>% 
  mutate(
    bathrooms = ifelse(!is.na(bathrooms), round(bathrooms), bathrooms),
    bedrooms  = ifelse(!is.na(bedrooms),  round(bedrooms),  bedrooms)
  )

# Redondear en test
test_final <- test_final %>% 
  mutate(
    bathrooms = ifelse(!is.na(bathrooms), round(bathrooms), bathrooms),
    bedrooms  = ifelse(!is.na(bedrooms),  round(bedrooms),  bedrooms)
  )

#Exportar


# Transformar a geogr√°fico
train_final <- st_transform(train_final, 4326)
test_final  <- st_transform(test_final, 4326)

# Extraer coordenadas
train_xy <- st_coordinates(train_final)
test_xy  <- st_coordinates(test_final)

# Asegurar que las variables queden correctas:
# X = longitud
# Y = latitud
train_final$longitude <- train_xy[, "X"]
train_final$latitude  <- train_xy[, "Y"]

test_final$longitude <- test_xy[, "X"]
test_final$latitude  <- test_xy[, "Y"]

# Eliminar geometr√≠a
train_final <- train_final %>% st_drop_geometry()
test_final  <- test_final  %>% st_drop_geometry()

fix_lon_lat <- function(df) {
  lon <- df$latitude
  lat <- df$longitude
  df$longitude <- lon
  df$latitude  <- lat
  df
}

train_final<- fix_lon_lat(train_final)
test_final  <- fix_lon_lat(test_final)



# Ruta din√°mica


output_dir <- here::here("data", "processed")

if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)

write.csv(train_final,
          file.path(output_dir, "train_final.csv"),
          row.names = FALSE)

write.csv(test_final,
          file.path(output_dir, "test_final.csv"),
          row.names = FALSE)

saveRDS(
  train_final,
  file.path(output_dir, "train_final.rds")
)

saveRDS(
  test_final,
  file.path(output_dir, "test_final.rds")
)

```



```{r}


# ===============================
# 7) Distribuciones (gr√°ficos sencillos)
# ===============================

# 7.1 Histograma para cada num√©rica (train)
if (length(num_cols) > 0) {
  train %>%
    select(all_of(num_cols)) %>%
    pivot_longer(everything(), names_to = "variable", values_to = "valor") %>%
    ggplot(aes(x = valor)) +
    geom_histogram(bins = 30) +
    facet_wrap(~ variable, scales = "free", ncol = 3) +
    labs(title = "Histogramas (TRAIN)", x = NULL, y = "Frecuencia")
}

# 7.2 Densidad del target (train), si existe
if (target %in% names(train)) {
  # Densidad en escala original
  ggplot(train, aes(x = .data[[target]])) +
    geom_density() +
    labs(title = paste("Densidad de", target, "(TRAIN)"), x = target, y = "Densidad")

  # Densidad en log (opcional)
  if (all(is.finite(train[[target]]), na.rm = TRUE)) {
    ggplot(train %>% filter(.data[[target]] > 0), aes(x = log1p(.data[[target]]))) +
      geom_density() +
      labs(title = paste("Densidad de log1p(", target, ") (TRAIN)"), x = paste0("log1p(", target, ")"), y = "Densidad")
  }
}

# 7.3 Boxplots del target por algunas categ√≥ricas (top k categor√≠as m√°s frecuentes)
top_k_levels <- function(x, k = 12) {
  forcats::fct_lump_n(x, n = k, other_level = "otros")
}

if (target %in% names(train) && length(cat_cols) > 0) {
  for (cc in cat_cols) {
    p <- train %>%
      mutate(..grp = top_k_levels(.data[[cc]], k = 12)) %>%
      ggplot(aes(x = ..grp, y = .data[[target]])) +
      geom_boxplot(outlier.alpha = 0.2) +
      coord_flip() +
      labs(title = paste("Boxplot de", target, "por", cc, "(TRAIN)"),
           x = cc, y = target)
    print(p)
  }
}

# 7.4 Pares de variables (scatter) para algunas num√©ricas clave
pairs_cols <- intersect(num_cols, c("bathrooms","bedrooms","rooms","surface_total","surface_covered","lat","lon","year","month"))
if (length(pairs_cols) >= 2 && target %in% names(train)) {
  df_pairs <- train %>% select(all_of(pairs_cols), all_of(target)) %>% drop_na()
  GGally::ggpairs(df_pairs, columns = 1:length(pairs_cols),
                  title = "Matriz de pares (subset num√©rico) - TRAIN")
}


# Correlaciones num√©ricas

if (length(num_cols) >= 2) {
  # correlaciones entre num√©ricas (train)
  cormat <- train %>% select(all_of(num_cols)) %>%
    mutate(across(everything(), as.numeric)) %>%
    cor(use = "pairwise.complete.obs")

  ggcorrplot::ggcorrplot(cormat, lab = FALSE, type = "lower") +
    ggtitle("Correlaci√≥n entre variables num√©ricas (TRAIN)")

# correlaci√≥n con target si num√©rico
  if (target %in% names(train) && is.numeric(train[[target]])) {
    cor_with_target <- train %>%
      select(all_of(num_cols), all_of(target)) %>%
      cor(use = "pairwise.complete.obs") %>%
      as.data.frame() %>%
      rownames_to_column("variable") %>%
      select(variable, all_of(target)) %>%
      arrange(desc(abs(.data[[target]])))
    cat("\nCorrelaci√≥n con el target (TRAIN):\n")
    print(cor_with_target)
  }
}


# columnas a modelar

keep_features <- setdiff(cols_common, target)  # lo com√∫n menos la y
cat("\nFeatures a usar (comunes, sin target):\n")
print(keep_features)
```




